#!/usr/bin/env python
import os
import sys
import ctypes
from primesense import openni2
from primesense import nite2
import cv2 
import rospy
import os
import roslib
import time
from std_msgs.msg import Bool
from cv_bridge import CvBridge, CvBridgeError
from sensor_msgs.msg import Image

class Face_Service():
        def __init__(self):
                self.bridge = CvBridge()
                self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback, queue_size = 1)
                self.cascPath = os.path.join("/home/robot/Robot/src/backup/ros_kit/skk/config/","haarcascade_frontalface_default.xml")
                self.faceCascade = cv2.CascadeClassifier(self.cascPath)

        def image_callback(self, image):
                try:
                    cv_image = self.bridge.imgmsg_to_cv2(image, "bgr8")
                    detection = self.detect_faces(cv_image)
                    print detection
                    for (x, y, w, h) in detection:
                        cv2.rectangle(cv_image, (x, y), (x+w, y+h), (0, 255, 0), 2)

                    cv2.imshow("Face", cv_image)
                    cv2.waitKey(1)
                except CvBridgeError, e:
                    print e

        def detect_faces(self, image):
                _scale_Factor=1.2
                _min_Neighbor=5
                _min_Size=(30,30)
                flags = cv2.cv.CV_HAAR_SCALE_IMAGE
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                faces = self.faceCascade.detectMultiScale(
                                gray,
                                _scale_Factor,
                                _min_Neighbor,
                                flags,
                                _min_Size
                                )

nite2.initialize()
ut=nite2.UserTracker.open_any()
while(1):
	frame = ut.read_frame()
	depth_frame = frame.get_depth_frame()
	for fu in frame.users:
	    users = fu
	    if users.is_new():
		ut.start_skeleton_tracking(fu.id)
		print users.skeleton.state
	    elif users.skeleton.state == 2:
		head = users.skeleton.joints[0]
		if(head.positionConfidence > 0.5):
		    print fu.id
		    print head.position.x/1000.0
		    print head.position.y/1000.0
		    #Depth direction
		    print head.position.z/1000.0

